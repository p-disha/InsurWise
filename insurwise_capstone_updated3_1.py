# -*- coding: utf-8 -*-
"""InsurWise_Capstone_Updated3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rfiUqJQeU0-x5KimYT-G6EovdpE5vR2v

# InsurWise: Generative AI Insurance Assistant

**What & Why**  
InsurWise helps Indian policyholders instantly query lengthy PDF policies—no more manually hunting through 40+ pages for “exclusions,” “waiting period,” or “claim steps.”

**How It Works**  
1. **Document Understanding**: Extract & chunk PDF text  
2. **Embeddings & Vector Search**: Semantic encoding + cosine similarity  
3. **Retrieval‑Augmented Generation (RAG)**: Fetch top‑k chunks → Gemini 1.5 Pro  
4. **Memory & Caching:** keep chat history and cache embeddings
5. **Interactive Q&A**: Free‑form questions → precise answers  

**GenAI Capabilities Used**  
- Document understanding (PDF parsing & chunking)  
- Embeddings & vector search  
- RAG (contextual LLM answering)  
- (Future) Function calling / Agents

**Embeddings (Vector Search)** – we turn each policy chunk into a vector so we can pull only the most relevant context.

**RAG (Retrieval‑Augmented Gen)**– we feed those top chunks + user Q to Gemini so the model never hallucinates on unrelated clauses.

**Context‑Caching**– we cache embeddings with @st.cache_data so re‑runs are instantaneous.

### PDF Preprocessing Assumptions

- **Text‑based PDFs only.** Scanned/image‑only docs require OCR (e.g., Tesseract).  
- We split at ~1,000 characters (sentence‑aware) to balance context vs token limits.  
- Multi‑column or table layouts may flatten—use clean policy docs for best results.
"""

# ─────────────────────────────────────────────────────────────────────────────
!pip install -q streamlit PyMuPDF google-generativeai scikit-learn

import streamlit as st
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
# … plus any other imports you already had (fitz/pdfplumber, etc.)
# ─────────────────────────────────────────────────────────────────────────────

!pip install -q PyMuPDF

import fitz  # PyMuPDF
# We assume a text‑based PDF with paragraphs separated by double newlines.
# If your PDF uses single newlines, you may need to post‑process accordingly.

def extract_pdf_chunks(pdf_path, max_chunk_chars=1000):
    doc = fitz.open(pdf_path)
    full_text = ""
    for page in doc:
        full_text += page.get_text("text")
    chunks = []
    while full_text:
        chunk = full_text[:max_chunk_chars]
        split_at = chunk.rfind(". ")
        sp = split_at + 1 if split_at > 0 else max_chunk_chars
        chunks.append(full_text[:sp].strip())
        full_text = full_text[sp:].strip()
    return [c for c in chunks if c]

# ─────────────────────────────────────────────────────────────
# Cell: Define embed_chunks with a real embedding model

from google.colab import userdata
import google.generativeai as genai
import numpy as np

# (Re‑configure just to be safe)
api_key = userdata.get("GOOGLE_API_KEY")
genai.configure(api_key=api_key)

# Choose one of the models you saw in list_models()
EMBED_MODEL = "models/text-embedding-004"

def embed_chunks(chunks):
    """Convert text chunks into embeddings for retrieval."""
    embs = []
    for chunk in chunks:
        resp = genai.embed_content(
            model=EMBED_MODEL,
            content=chunk,
            task_type="RETRIEVAL_DOCUMENT"
        )
        embs.append(resp["embedding"])
    return np.array(embs)
# ─────────────────────────────────────────────────────────────

# ─────────────────────────────────────────────────────────────
# Cell: Upload, chunk & embed

from google.colab import userdata, files
import google.generativeai as genai
import numpy as np

# 1️⃣ Fetch & configure your real API key
api_key = userdata.get("GOOGLE_API_KEY")
if not api_key:
    raise RuntimeError(
        "❌ Colab secret named 'GOOGLE_API_KEY' not found. "
        "Go to Runtime → Manage secrets, add it, and toggle it ON."
    )
genai.configure(api_key=api_key)

# 2️⃣ Upload & chunk
uploaded = files.upload()                   # pick your PDF
pdf_path = next(iter(uploaded.keys()))
chunks = extract_pdf_chunks(pdf_path)       # your PDF→chunks fn
assert chunks, "No text extracted from PDF!"

# 3️⃣ Embed
chunk_embeddings = embed_chunks(chunks)     # uses your embed_chunks fn
print("✅ Done. Embedding matrix shape:", chunk_embeddings.shape)
# ─────────────────────────────────────────────────────────────

# ─────────────────────────────────────────────────────────────────────────────
# Cell: Define rag_ask for RAG‐based Q&A

from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai

# Make sure your embedding model matches the one used above
EMBED_MODEL = "models/text-embedding-004"

def rag_ask(
    question: str,
    chunks: list[str],
    chunk_embeddings: np.ndarray,
    history: list[dict] = None,
    model_name: str = "gemini-1.5-pro-latest",
    top_k: int = 5,
    default_q: str = "What are the exclusions in this policy?"
) -> str:
    # 0️⃣ Default question if empty
    if not question.strip():
        question = default_q

    # 1️⃣ Build a small memory of the last 3 turns
    mem = ""
    if history:
        recent = history[-3:]
        mem = "### Conversation History:\n"
        for turn in recent:
            mem += f"Q: {turn['q']}\nA: {turn['a']}\n"
        mem += "\n"

    # 2️⃣ Embed the question
    resp = genai.embed_content(
        model=EMBED_MODEL,
        content=question,
        task_type="RETRIEVAL_QUERY"
    )
    q_vec = resp["embedding"]

    # 3️⃣ Retrieve top‐k similar chunks
    sims = cosine_similarity([q_vec], chunk_embeddings)[0]
    top_idxs = sims.argsort()[-top_k:][::-1]
    context = "\n\n".join(chunks[i] for i in top_idxs)

    # 4️⃣ Assemble prompt
    prompt = f"""You are an insurance‐policy expert.
Use ONLY the context below and prior conversation to answer.

{mem}
--- Policy Context ---
{context}

Question: {question}
Answer:"""

    # 5️⃣ Generate answer
    model = genai.GenerativeModel(model_name)
    return model.generate_content(prompt).text.strip()
# ─────────────────────────────────────────────────────────────────────────────

# ─────────────────────────────────────────────────────────────
# Cell: Interactive Q&A in the notebook

# initialize a history list
history = []

print("🛡️ Welcome to InsurWise Notebook Q&A!")
print("Type your question and press Enter (or 'exit' to stop).")

while True:
    try:
        q = input("\n❓ Your question: ")
    except (EOFError, KeyboardInterrupt):
        print("\n👋 Exiting Q&A.")
        break

    # if user just hits enter, use the default exclusions question
    if q.strip() == "":
        default_q: str = "What are the exclusions in this policy?"
        q = default_q
        print(f"🔄 Using default question: {q}")

    # quit only on explicit 'q' or 'exit'
    if q.strip().lower() in ("q", "exit"):
        print("👋 Exiting Q&A.")
        break

    # call RAG
    a = rag_ask(q, chunks, chunk_embeddings, history)

    # record and display
    history.append({"q": q, "a": a})
    print(f"\n📝 Answer:\n{a}")

# Optionally print full history at the end
print("\n📜 Full Q&A history:")
for turn in history:
    print(f"Q: {turn['q']}\nA: {turn['a']}\n---")
# ─────────────────────────────────────────────────────────────

"""### Direct LLM vs. RAG‑Enhanced

- **Direct LLM**: dumps large prompt → token limits & hallucinations  
- **RAG**: retrieves only relevant snippets → precise, efficient answers
"""

# ─────────────────────────────────────────────────────────────────────────────
# ⚖️  Baseline vs. RAG Comparison

# pretend no chat history
history = []

# rebuild full document
full_text = "\n\n".join(chunks)
test_q   = "What are the exclusions in this policy?"

# 1) Vanilla baseline
baseline_ctx    = full_text[:2000]
baseline_prompt = f"""
You are an insurance‐policy expert. Use ONLY the text below to answer.

{baseline_ctx}

Question: {test_q}
Answer:
"""
baseline_resp = genai.GenerativeModel("gemini-1.5-pro-latest") \
                  .generate_content(baseline_prompt).text.strip()
print("🔍 Baseline answer:\n", baseline_resp)

# 2) RAG answer
rag_resp = rag_ask(test_q, chunks, chunk_embeddings, history)
print("\n🔍 RAG answer:\n", rag_resp)
# ─────────────────────────────────────────────────────────────────────────────

import re
def summarize_exclusions(chunks):
    raw = "\n\n".join([c for c in chunks if re.search(r'\bexclusion\b', c, re.I)])
    if not raw: return "No exclusions found."
    prompt = f"... bullet‑list summarizer ..."
    return genai.GenerativeModel("gemini-1.5-pro-latest") \
        .generate_content(prompt).text.strip()
print(summarize_exclusions(chunks))

# ─────────────────────────────────────────────────────────────────────────────
st.set_page_config(
    page_title="🛡️ InsurWise Q&A",
    layout="wide",
    initial_sidebar_state="expanded"
)
st.title("🛡️ InsurWise: Insurance Policy Q&A")
st.markdown(
    "Upload a policy PDF in the sidebar, then ask questions—"
    "the app uses RAG+Gemini to answer with context and memory."
)
# ─────────────────────────────────────────────────────────────────────────────

# ─────────────────────────────────────────────────────────────────────────────

with st.sidebar:
    st.header("1️⃣ Upload your policy PDF")
    pdf_file = st.file_uploader("PDF file", type="pdf")
    if pdf_file:
        try:
            # assume extract_pdf_chunks can accept bytes
            chunks = extract_pdf_chunks(pdf_file.read())
            if not chunks:
                st.error("No text found—ensure the PDF is text‑based.")
                st.stop()
            st.success(f"Extracted {len(chunks)} chunks")
        except Exception as e:
            st.error(f"Failed to parse PDF: {e}")
            st.stop()
    else:
        st.info("Awaiting upload…")
        st.stop()

# ─────────────────────────────────────────────────────────────────────────────

# ─────────────────────────────────────────────────────────────────────────────
# 📦 Create a secrets.toml in Colab for Streamlit to pick up your API key
import os

# Colab secrets live in userdata, not env‐vars
from google.colab import userdata
api_key = userdata.get("GOOGLE_API_KEY")   # must match exactly the “Name” in Settings→Variables
if not api_key:
    raise RuntimeError(
        "❌ No Colab secret named 'GOOGLE_API_KEY'.\n"
        "Go to Runtime → Manage variables, add a variable exactly called "
        "'GOOGLE_API_KEY', toggle it ON, then re‑run this cell."
    )

# Write the Streamlit secrets.toml
secret_dir = os.path.expanduser("~/.streamlit")
os.makedirs(secret_dir, exist_ok=True)
with open(os.path.join(secret_dir, "secrets.toml"), "w") as f:
    f.write(f"""
[general]
GOOGLE_API_KEY = "{api_key}"
""".lstrip())

print("✅ Wrote ~/.streamlit/secrets.toml")
# ─────────────────────────────────────────────────────────────────────────────

!pip install pyngrok --quiet
from pyngrok import ngrok
ngrok.kill()   # ← wipes out all active tunnels

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > app.py << 'EOF'
# import streamlit as st
# import numpy as np
# from sklearn.metrics.pairwise import cosine_similarity
# import google.generativeai as genai
# import fitz  # PyMuPDF
# import google.api_core.exceptions as ga_exc
# 
# # ── 1️⃣ Configure your API key
# api_key = st.secrets["GOOGLE_API_KEY"]
# genai.configure(api_key=api_key)
# 
# # ── 2️⃣ PDF‑to‑chunks utility with automatic sub‑chunking
# def extract_chunks(pdf_bytes, max_chars=1000):
#     doc = fitz.open(stream=pdf_bytes, filetype="pdf")
#     text = ""
#     for page in doc:
#         text += page.get_text()
#     # split on double‑newlines, collapse internal newlines for readability
#     paras = [p.replace("\\n", " ").strip() for p in text.split("\\n\\n") if p.strip()]
#     chunks = []
#     for p in paras:
#         if len(p) <= max_chars:
#             chunks.append(p)
#         else:
#             # break oversized paragraph into sub‑chunks
#             start = 0
#             while start < len(p):
#                 end = min(start + max_chars, len(p))
#                 # try to split on last space before end
#                 if end < len(p):
#                     space = p.rfind(" ", start, end)
#                     if space > start:
#                         end = space
#                 chunks.append(p[start:end])
#                 start = end
#     return chunks
# 
# # ── 3️⃣ Cache embeddings with payload‑size error handling
# EMBED_MODEL = "models/text-embedding-004"
# 
# @st.cache_data
# def get_embeddings(chunks):
#     embs = []
#     for idx, c in enumerate(chunks):
#         if not c.strip():
#             continue
#         try:
#             resp = genai.embed_content(
#                 model=EMBED_MODEL,
#                 content=c,
#                 task_type="RETRIEVAL_DOCUMENT"
#             )
#         except ga_exc.InvalidArgument as e:
#             raise RuntimeError(
#                 f"Chunk #{idx} is too large for the embedding API (>{EMBED_MODEL} limit). "
#                 "Try reducing max_chars in extract_chunks."
#             )
#         embs.append(resp["embedding"])
#     return np.array(embs)
# 
# # ── 4️⃣ RAG + Memory QA function
# def rag_ask(question, chunks, chunk_embeddings, history,
#             model_name="gemini-1.5-pro-latest", top_k=5,
#             default_q="What are the exclusions in this policy?"):
#     if not question.strip():
#         question = default_q
# 
#     # assemble last 3 turns of history
#     mem = ""
#     for turn in history[-3:]:
#         mem += f"Q: {turn['q']}\nA: {turn['a']}\n"
#     if mem:
#         mem = "### History:\n" + mem + "\n"
# 
#     # embed the question
#     qv = genai.embed_content(
#         model=EMBED_MODEL,
#         content=question,
#         task_type="RETRIEVAL_QUERY"
#     )["embedding"]
# 
#     # retrieve top‑k chunks
#     sims = cosine_similarity([qv], chunk_embeddings)[0]
#     idxs = sims.argsort()[-top_k:][::-1]
#     ctx = "\n\n".join(chunks[i] for i in idxs)
# 
#     # triple‑quoted prompt must start at column 0
#     prompt = f"""You are an insurance-policy expert.
# Use ONLY the context below and prior conversation to answer.
# 
# {mem}
# --- Policy Context ---
# {ctx}
# 
# Question: {question}
# Answer:"""
# 
#     return genai.GenerativeModel(model_name).generate_content(prompt).text.strip()
# 
# # ── 5️⃣ Streamlit UI
# st.set_page_config(page_title="InsurWise Q&A", layout="wide")
# st.title("🛡️ InsurWise: Policy Q&A")
# 
# # PDF upload
# pdf_file = st.file_uploader("Upload your policy PDF", type="pdf")
# if not pdf_file:
#     st.info("Please upload a PDF to get started.")
#     st.stop()
# 
# # chunk & validate
# chunks = extract_chunks(pdf_file.read())
# chunks = [c for c in chunks if c.strip()]
# if not chunks:
#     st.error("❌ No text could be extracted. Make sure it’s a text‑based PDF.")
#     st.stop()
# 
# # embed
# chunk_embeddings = get_embeddings(chunks)
# st.success(f"✅ Parsed into {len(chunks)} chunks and embedded.")
# 
# # chat history
# if "history" not in st.session_state:
#     st.session_state.history = []
# 
# q = st.text_input("Ask a question about your policy:", "")
# if st.button("Ask"):
#     with st.spinner("🔍 Thinking…"):
#         a = rag_ask(q, chunks, chunk_embeddings, st.session_state.history)
#     st.session_state.history.append({"q": q or "<default>", "a": a})
# 
# # display
# for turn in st.session_state.history:
#     st.markdown(f"**Q:** {turn['q']}")
#     st.markdown(f"**A:** {turn['a']}\n---")
# EOF
#

# ── 0️⃣ Dump your Colab secret into Streamlit’s secrets.toml ─────────────────
from google.colab import userdata
import os

# 1) Grab the key from Colab Secrets (Settings → Variables)
api_key = userdata.get("GOOGLE_API_KEY")
if not api_key:
    raise RuntimeError(
        "❌ Colab secret named 'GOOGLE_API_KEY' not found. "
        "Go to Runtime → Manage secrets, add it, and toggle it ON."
    )

# 2) Create both of Streamlit’s lookup dirs
for secrets_dir in [
    os.path.expanduser("~/.streamlit"),
    "/content/.streamlit",
]:
    os.makedirs(secrets_dir, exist_ok=True)
    with open(os.path.join(secrets_dir, "secrets.toml"), "w") as f:
        # note: no [general] section needed for Community Cloud
        f.write(f'GOOGLE_API_KEY = "{api_key}"\n')

print("✅ Wrote your key to ~/.streamlit/secrets.toml and /content/.streamlit/secrets.toml")

# Commented out IPython magic to ensure Python compatibility.
# # 1) Install everything
# %%bash
# pip install --quiet streamlit pyngrok
# 
# # 2) Launch Streamlit in the background
# #    - nohup + & will let the cell finish immediately
# nohup streamlit run app.py \
#      --server.headless true \
#      --server.port 8501 \
#   &>/dev/null &
# 
# echo "▶️ Streamlit started on port 8501"
#

from google.colab import userdata
from pyngrok import ngrok

# 1️⃣ Fetch your ngrok token from Colab Variables
auth_token = userdata.get("NGROK_AUTH_TOKEN")
if not auth_token:
    raise ValueError(
      "❌ No ngrok authtoken found. "
      "Go to Settings→Variables, add NGROK_AUTH_TOKEN, and toggle it ON."
    )

# 2️⃣ Configure ngrok with your token
ngrok.set_auth_token(auth_token)

# 3️⃣ Now open your Streamlit port
public_url = ngrok.connect(8501, "http")
print("🌍 Your Streamlit app is live at:", public_url)

"""## Conclusion & Next Steps

InsurWise showcases document understanding, embeddings, RAG, and (future) function‑calling to build an intelligent insurance assistant.

**Next Steps**  
- Implement real function‑calling agents (e.g. `file_claim`, `get_best_policy`)  
- Deploy as a web app (Streamlit / FastAPI)  
- Extend to multi‑policy comparison and IRDAI grounding  

Happy coding & good luck in the competition! 🚀
"""