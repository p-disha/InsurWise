# -*- coding: utf-8 -*-
"""InsurWise_Capstone_Updated3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rfiUqJQeU0-x5KimYT-G6EovdpE5vR2v

# InsurWise: Generative AI Insurance Assistant

**What & Why**  
InsurWise helps Indian policyholders instantly query lengthy PDF policies‚Äîno more manually hunting through 40+ pages for ‚Äúexclusions,‚Äù ‚Äúwaiting period,‚Äù or ‚Äúclaim steps.‚Äù

**How It Works**  
1. **Document Understanding**: Extract & chunk PDF text  
2. **Embeddings & Vector Search**: Semantic encoding + cosine similarity  
3. **Retrieval‚ÄëAugmented Generation (RAG)**: Fetch top‚Äëk chunks ‚Üí Gemini 1.5¬†Pro  
4. **Memory & Caching:** keep chat history and cache embeddings
5. **Interactive Q&A**: Free‚Äëform questions ‚Üí precise answers  

**GenAI Capabilities Used**  
- Document understanding (PDF parsing & chunking)  
- Embeddings & vector search  
- RAG (contextual LLM answering)  
- (Future) Function calling / Agents

**Embeddings (Vector Search)** ‚Äì we turn each policy chunk into a vector so we can pull only the most relevant context.

**RAG (Retrieval‚ÄëAugmented Gen)**‚Äì we feed those top chunks + user Q to Gemini so the model never hallucinates on unrelated clauses.

**Context‚ÄëCaching**‚Äì we cache embeddings with @st.cache_data so re‚Äëruns are instantaneous.

### PDF Preprocessing Assumptions

- **Text‚Äëbased PDFs only.** Scanned/image‚Äëonly docs require OCR (e.g., Tesseract).  
- We split at ~1,000 characters (sentence‚Äëaware) to balance context vs token limits.  
- Multi‚Äëcolumn or table layouts may flatten‚Äîuse clean policy docs for best results.
"""

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
!pip install -q streamlit PyMuPDF google-generativeai scikit-learn

import streamlit as st
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
# ‚Ä¶ plus any other imports you already had (fitz/pdfplumber, etc.)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

!pip install -q PyMuPDF

import fitz  # PyMuPDF
# We assume a text‚Äëbased PDF with paragraphs separated by double newlines.
# If your PDF uses single newlines, you may need to post‚Äëprocess accordingly.

def extract_pdf_chunks(pdf_path, max_chunk_chars=1000):
    doc = fitz.open(pdf_path)
    full_text = ""
    for page in doc:
        full_text += page.get_text("text")
    chunks = []
    while full_text:
        chunk = full_text[:max_chunk_chars]
        split_at = chunk.rfind(". ")
        sp = split_at + 1 if split_at > 0 else max_chunk_chars
        chunks.append(full_text[:sp].strip())
        full_text = full_text[sp:].strip()
    return [c for c in chunks if c]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Cell: Define embed_chunks with a real embedding model

from google.colab import userdata
import google.generativeai as genai
import numpy as np

# (Re‚Äëconfigure just to be safe)
api_key = userdata.get("GOOGLE_API_KEY")
genai.configure(api_key=api_key)

# Choose one of the models you saw in list_models()
EMBED_MODEL = "models/text-embedding-004"

def embed_chunks(chunks):
    """Convert text chunks into embeddings for retrieval."""
    embs = []
    for chunk in chunks:
        resp = genai.embed_content(
            model=EMBED_MODEL,
            content=chunk,
            task_type="RETRIEVAL_DOCUMENT"
        )
        embs.append(resp["embedding"])
    return np.array(embs)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Cell: Upload, chunk & embed

from google.colab import userdata, files
import google.generativeai as genai
import numpy as np

# 1Ô∏è‚É£ Fetch & configure your real API key
api_key = userdata.get("GOOGLE_API_KEY")
if not api_key:
    raise RuntimeError(
        "‚ùå Colab secret named 'GOOGLE_API_KEY' not found. "
        "Go to Runtime ‚Üí Manage secrets, add it, and toggle it ON."
    )
genai.configure(api_key=api_key)

# 2Ô∏è‚É£ Upload & chunk
uploaded = files.upload()                   # pick your PDF
pdf_path = next(iter(uploaded.keys()))
chunks = extract_pdf_chunks(pdf_path)       # your PDF‚Üíchunks fn
assert chunks, "No text extracted from PDF!"

# 3Ô∏è‚É£ Embed
chunk_embeddings = embed_chunks(chunks)     # uses your embed_chunks fn
print("‚úÖ Done. Embedding matrix shape:", chunk_embeddings.shape)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Cell: Define rag_ask for RAG‚Äêbased Q&A

from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai

# Make sure your embedding model matches the one used above
EMBED_MODEL = "models/text-embedding-004"

def rag_ask(
    question: str,
    chunks: list[str],
    chunk_embeddings: np.ndarray,
    history: list[dict] = None,
    model_name: str = "gemini-1.5-pro-latest",
    top_k: int = 5,
    default_q: str = "What are the exclusions in this policy?"
) -> str:
    # 0Ô∏è‚É£ Default question if empty
    if not question.strip():
        question = default_q

    # 1Ô∏è‚É£ Build a small memory of the last 3 turns
    mem = ""
    if history:
        recent = history[-3:]
        mem = "### Conversation History:\n"
        for turn in recent:
            mem += f"Q: {turn['q']}\nA: {turn['a']}\n"
        mem += "\n"

    # 2Ô∏è‚É£ Embed the question
    resp = genai.embed_content(
        model=EMBED_MODEL,
        content=question,
        task_type="RETRIEVAL_QUERY"
    )
    q_vec = resp["embedding"]

    # 3Ô∏è‚É£ Retrieve top‚Äêk similar chunks
    sims = cosine_similarity([q_vec], chunk_embeddings)[0]
    top_idxs = sims.argsort()[-top_k:][::-1]
    context = "\n\n".join(chunks[i] for i in top_idxs)

    # 4Ô∏è‚É£ Assemble prompt
    prompt = f"""You are an insurance‚Äêpolicy expert.
Use ONLY the context below and prior conversation to answer.

{mem}
--- Policy Context ---
{context}

Question: {question}
Answer:"""

    # 5Ô∏è‚É£ Generate answer
    model = genai.GenerativeModel(model_name)
    return model.generate_content(prompt).text.strip()
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Cell: Interactive Q&A in the notebook

# initialize a history list
history = []

print("üõ°Ô∏è Welcome to InsurWise Notebook Q&A!")
print("Type your question and press Enter (or 'exit' to stop).")

while True:
    try:
        q = input("\n‚ùì Your question: ")
    except (EOFError, KeyboardInterrupt):
        print("\nüëã Exiting Q&A.")
        break

    # if user just hits enter, use the default exclusions question
    if q.strip() == "":
        default_q: str = "What are the exclusions in this policy?"
        q = default_q
        print(f"üîÑ Using default question: {q}")

    # quit only on explicit 'q' or 'exit'
    if q.strip().lower() in ("q", "exit"):
        print("üëã Exiting Q&A.")
        break

    # call RAG
    a = rag_ask(q, chunks, chunk_embeddings, history)

    # record and display
    history.append({"q": q, "a": a})
    print(f"\nüìù Answer:\n{a}")

# Optionally print full history at the end
print("\nüìú Full Q&A history:")
for turn in history:
    print(f"Q: {turn['q']}\nA: {turn['a']}\n---")
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

"""### Direct LLM vs. RAG‚ÄëEnhanced

- **Direct LLM**: dumps large prompt ‚Üí token limits & hallucinations  
- **RAG**: retrieves only relevant snippets ‚Üí precise, efficient answers
"""

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚öñÔ∏è  Baseline vs. RAG Comparison

# pretend no chat history
history = []

# rebuild full document
full_text = "\n\n".join(chunks)
test_q   = "What are the exclusions in this policy?"

# 1) Vanilla baseline
baseline_ctx    = full_text[:2000]
baseline_prompt = f"""
You are an insurance‚Äêpolicy expert. Use ONLY the text below to answer.

{baseline_ctx}

Question: {test_q}
Answer:
"""
baseline_resp = genai.GenerativeModel("gemini-1.5-pro-latest") \
                  .generate_content(baseline_prompt).text.strip()
print("üîç Baseline answer:\n", baseline_resp)

# 2) RAG answer
rag_resp = rag_ask(test_q, chunks, chunk_embeddings, history)
print("\nüîç RAG answer:\n", rag_resp)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

import re
def summarize_exclusions(chunks):
    raw = "\n\n".join([c for c in chunks if re.search(r'\bexclusion\b', c, re.I)])
    if not raw: return "No exclusions found."
    prompt = f"... bullet‚Äëlist summarizer ..."
    return genai.GenerativeModel("gemini-1.5-pro-latest") \
        .generate_content(prompt).text.strip()
print(summarize_exclusions(chunks))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(
    page_title="üõ°Ô∏è InsurWise Q&A",
    layout="wide",
    initial_sidebar_state="expanded"
)
st.title("üõ°Ô∏è InsurWise: Insurance Policy Q&A")
st.markdown(
    "Upload a policy PDF in the sidebar, then ask questions‚Äî"
    "the app uses RAG+Gemini to answer with context and memory."
)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

with st.sidebar:
    st.header("1Ô∏è‚É£ Upload your policy PDF")
    pdf_file = st.file_uploader("PDF file", type="pdf")
    if pdf_file:
        try:
            # assume extract_pdf_chunks can accept bytes
            chunks = extract_pdf_chunks(pdf_file.read())
            if not chunks:
                st.error("No text found‚Äîensure the PDF is text‚Äëbased.")
                st.stop()
            st.success(f"Extracted {len(chunks)} chunks")
        except Exception as e:
            st.error(f"Failed to parse PDF: {e}")
            st.stop()
    else:
        st.info("Awaiting upload‚Ä¶")
        st.stop()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üì¶ Create a secrets.toml in Colab for Streamlit to pick up your API key
import os

# Colab secrets live in userdata, not env‚Äêvars
from google.colab import userdata
api_key = userdata.get("GOOGLE_API_KEY")   # must match exactly the ‚ÄúName‚Äù in Settings‚ÜíVariables
if not api_key:
    raise RuntimeError(
        "‚ùå No Colab secret named 'GOOGLE_API_KEY'.\n"
        "Go to Runtime ‚Üí Manage variables, add a variable exactly called "
        "'GOOGLE_API_KEY', toggle it ON, then re‚Äërun this cell."
    )

# Write the Streamlit secrets.toml
secret_dir = os.path.expanduser("~/.streamlit")
os.makedirs(secret_dir, exist_ok=True)
with open(os.path.join(secret_dir, "secrets.toml"), "w") as f:
    f.write(f"""
[general]
GOOGLE_API_KEY = "{api_key}"
""".lstrip())

print("‚úÖ Wrote ~/.streamlit/secrets.toml")
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

!pip install pyngrok --quiet
from pyngrok import ngrok
ngrok.kill()   # ‚Üê wipes out all active tunnels

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > app.py << 'EOF'
# import streamlit as st
# import numpy as np
# from sklearn.metrics.pairwise import cosine_similarity
# import google.generativeai as genai
# import fitz  # PyMuPDF
# import google.api_core.exceptions as ga_exc
# 
# # ‚îÄ‚îÄ 1Ô∏è‚É£ Configure your API key
# api_key = st.secrets["GOOGLE_API_KEY"]
# genai.configure(api_key=api_key)
# 
# # ‚îÄ‚îÄ 2Ô∏è‚É£ PDF‚Äëto‚Äëchunks utility with automatic sub‚Äëchunking
# def extract_chunks(pdf_bytes, max_chars=1000):
#     doc = fitz.open(stream=pdf_bytes, filetype="pdf")
#     text = ""
#     for page in doc:
#         text += page.get_text()
#     # split on double‚Äënewlines, collapse internal newlines for readability
#     paras = [p.replace("\\n", " ").strip() for p in text.split("\\n\\n") if p.strip()]
#     chunks = []
#     for p in paras:
#         if len(p) <= max_chars:
#             chunks.append(p)
#         else:
#             # break oversized paragraph into sub‚Äëchunks
#             start = 0
#             while start < len(p):
#                 end = min(start + max_chars, len(p))
#                 # try to split on last space before end
#                 if end < len(p):
#                     space = p.rfind(" ", start, end)
#                     if space > start:
#                         end = space
#                 chunks.append(p[start:end])
#                 start = end
#     return chunks
# 
# # ‚îÄ‚îÄ 3Ô∏è‚É£ Cache embeddings with payload‚Äësize error handling
# EMBED_MODEL = "models/text-embedding-004"
# 
# @st.cache_data
# def get_embeddings(chunks):
#     embs = []
#     for idx, c in enumerate(chunks):
#         if not c.strip():
#             continue
#         try:
#             resp = genai.embed_content(
#                 model=EMBED_MODEL,
#                 content=c,
#                 task_type="RETRIEVAL_DOCUMENT"
#             )
#         except ga_exc.InvalidArgument as e:
#             raise RuntimeError(
#                 f"Chunk #{idx} is too large for the embedding API (>{EMBED_MODEL} limit). "
#                 "Try reducing max_chars in extract_chunks."
#             )
#         embs.append(resp["embedding"])
#     return np.array(embs)
# 
# # ‚îÄ‚îÄ 4Ô∏è‚É£ RAG + Memory QA function
# def rag_ask(question, chunks, chunk_embeddings, history,
#             model_name="gemini-1.5-pro-latest", top_k=5,
#             default_q="What are the exclusions in this policy?"):
#     if not question.strip():
#         question = default_q
# 
#     # assemble last 3 turns of history
#     mem = ""
#     for turn in history[-3:]:
#         mem += f"Q: {turn['q']}\nA: {turn['a']}\n"
#     if mem:
#         mem = "### History:\n" + mem + "\n"
# 
#     # embed the question
#     qv = genai.embed_content(
#         model=EMBED_MODEL,
#         content=question,
#         task_type="RETRIEVAL_QUERY"
#     )["embedding"]
# 
#     # retrieve top‚Äëk chunks
#     sims = cosine_similarity([qv], chunk_embeddings)[0]
#     idxs = sims.argsort()[-top_k:][::-1]
#     ctx = "\n\n".join(chunks[i] for i in idxs)
# 
#     # triple‚Äëquoted prompt must start at column 0
#     prompt = f"""You are an insurance-policy expert.
# Use ONLY the context below and prior conversation to answer.
# 
# {mem}
# --- Policy Context ---
# {ctx}
# 
# Question: {question}
# Answer:"""
# 
#     return genai.GenerativeModel(model_name).generate_content(prompt).text.strip()
# 
# # ‚îÄ‚îÄ 5Ô∏è‚É£ Streamlit UI
# st.set_page_config(page_title="InsurWise Q&A", layout="wide")
# st.title("üõ°Ô∏è InsurWise: Policy Q&A")
# 
# # PDF upload
# pdf_file = st.file_uploader("Upload your policy PDF", type="pdf")
# if not pdf_file:
#     st.info("Please upload a PDF to get started.")
#     st.stop()
# 
# # chunk & validate
# chunks = extract_chunks(pdf_file.read())
# chunks = [c for c in chunks if c.strip()]
# if not chunks:
#     st.error("‚ùå No text could be extracted. Make sure it‚Äôs a text‚Äëbased PDF.")
#     st.stop()
# 
# # embed
# chunk_embeddings = get_embeddings(chunks)
# st.success(f"‚úÖ Parsed into {len(chunks)} chunks and embedded.")
# 
# # chat history
# if "history" not in st.session_state:
#     st.session_state.history = []
# 
# q = st.text_input("Ask a question about your policy:", "")
# if st.button("Ask"):
#     with st.spinner("üîç Thinking‚Ä¶"):
#         a = rag_ask(q, chunks, chunk_embeddings, st.session_state.history)
#     st.session_state.history.append({"q": q or "<default>", "a": a})
# 
# # display
# for turn in st.session_state.history:
#     st.markdown(f"**Q:** {turn['q']}")
#     st.markdown(f"**A:** {turn['a']}\n---")
# EOF
#

# ‚îÄ‚îÄ 0Ô∏è‚É£ Dump your Colab secret into Streamlit‚Äôs secrets.toml ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from google.colab import userdata
import os

# 1) Grab the key from Colab Secrets (Settings ‚Üí Variables)
api_key = userdata.get("GOOGLE_API_KEY")
if not api_key:
    raise RuntimeError(
        "‚ùå Colab secret named 'GOOGLE_API_KEY' not found. "
        "Go to Runtime ‚Üí Manage secrets, add it, and toggle it ON."
    )

# 2) Create both of Streamlit‚Äôs lookup dirs
for secrets_dir in [
    os.path.expanduser("~/.streamlit"),
    "/content/.streamlit",
]:
    os.makedirs(secrets_dir, exist_ok=True)
    with open(os.path.join(secrets_dir, "secrets.toml"), "w") as f:
        # note: no [general] section needed for Community Cloud
        f.write(f'GOOGLE_API_KEY = "{api_key}"\n')

print("‚úÖ Wrote your key to ~/.streamlit/secrets.toml and /content/.streamlit/secrets.toml")

# Commented out IPython magic to ensure Python compatibility.
# # 1) Install everything
# %%bash
# pip install --quiet streamlit pyngrok
# 
# # 2) Launch Streamlit in the background
# #    - nohup + & will let the cell finish immediately
# nohup streamlit run app.py \
#      --server.headless true \
#      --server.port 8501 \
#   &>/dev/null &
# 
# echo "‚ñ∂Ô∏è Streamlit started on port 8501"
#

from google.colab import userdata
from pyngrok import ngrok

# 1Ô∏è‚É£ Fetch your ngrok token from Colab Variables
auth_token = userdata.get("NGROK_AUTH_TOKEN")
if not auth_token:
    raise ValueError(
      "‚ùå No ngrok authtoken found. "
      "Go to Settings‚ÜíVariables, add NGROK_AUTH_TOKEN, and toggle it ON."
    )

# 2Ô∏è‚É£ Configure ngrok with your token
ngrok.set_auth_token(auth_token)

# 3Ô∏è‚É£ Now open your Streamlit port
public_url = ngrok.connect(8501, "http")
print("üåç Your Streamlit app is live at:", public_url)

"""## Conclusion & Next Steps

InsurWise showcases document understanding, embeddings, RAG, and (future) function‚Äëcalling to build an intelligent insurance assistant.

**Next Steps**  
- Implement real function‚Äëcalling agents (e.g. `file_claim`, `get_best_policy`)  
- Deploy as a web app (Streamlit / FastAPI)  
- Extend to multi‚Äëpolicy comparison and IRDAI grounding  

Happy coding & good luck in the competition! üöÄ
"""